{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76916a2c-5f78-43c7-a8ff-086ff069b2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emmad\\OneDrive - University of Leeds\\leeds admin\\chem\\3650\\coding\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c0e9dea-9725-429f-8883-7a6246715af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 8770 rows\n",
      "Filtered dataset size: 4377 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"combined_data_no_duplicates_and_ave_duplicates.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Count occurrences of each solute and solvent\n",
    "solute_counts = df['solute_smiles'].value_counts()\n",
    "solvent_counts = df['solvent_smiles'].value_counts()\n",
    "\n",
    "# Filter out solutes and solvents that have fewer than 5 data points\n",
    "filtered_df = df[\n",
    "    (df['solute_smiles'].map(solute_counts) >= 5) & (df['solvent_smiles'].map(solvent_counts) >= 10)\n",
    "]\n",
    "\n",
    "# Save the cleaned dataset\n",
    "filtered_df.to_csv(\"filtered_data_updated.csv\", index=False)\n",
    "\n",
    "# Print before and after sizes\n",
    "print(f\"Original dataset size: {df.shape[0]} rows\")\n",
    "print(f\"Filtered dataset size: {filtered_df.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a21a446-55b3-460c-8efc-bc784414c0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 4377 rows\n",
      "Filtered dataset size: 4342 rows\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "file_path = \"filtered_data_updated.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the 'logS_M' column to numeric, coercing any non-numeric values to NaN\n",
    "df['logS_M'] = pd.to_numeric(df['logS_M'], errors='coerce')\n",
    "\n",
    "# Filter out rows where logS is below -10\n",
    "filtered_df = df[df['logS_M'] >= -10]\n",
    "\n",
    "# Save the cleaned dataset\n",
    "filtered_df.to_csv(\"filtered_data_updated2.csv\", index=False)\n",
    "\n",
    "# Print before and after sizes\n",
    "print(f\"Original dataset size: {df.shape[0]} rows\")\n",
    "print(f\"Filtered dataset size: {filtered_df.shape[0]} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b41b6c91-5431-495d-a5cf-0655dc0010f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (4342, 8)\n",
      "New shape after removal: (4295, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"filtered_data_updated2.csv\"  # Update with the correct path if needed\n",
    "filtered_data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove rows where 'solute_name' contains 'DONOTUSE'\n",
    "filtered_data = filtered_data[~filtered_data['solute_name'].str.contains('DONOTUSE', na=False)]\n",
    "\n",
    "# Save the cleaned data back to a new CSV file\n",
    "filtered_data.to_csv(\"filtered_data_updated3.csv\", index=False)\n",
    "\n",
    "# Print shape to confirm row removal\n",
    "print(f\"Original shape: {pd.read_csv(file_path).shape}\")\n",
    "print(f\"New shape after removal: {filtered_data.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd1a948b-e6e2-44f1-9600-e2dc16c91d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (4295, 8)\n",
      "New shape after removal: (4055, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"filtered_data_updated3.csv\"  # Update with the correct path if needed\n",
    "filtered_data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove rows where 'solute_name' contains 'DONOTUSE'\n",
    "filtered_data = filtered_data[~filtered_data['solvent_name'].str.contains('ethanol/water', na=False)]\n",
    "\n",
    "# Save the cleaned data back to a new CSV file\n",
    "filtered_data.to_csv(\"filtered_data_updated_4.csv\", index=False)\n",
    "\n",
    "# Print shape to confirm row removal\n",
    "print(f\"Original shape: {pd.read_csv(file_path).shape}\")\n",
    "print(f\"New shape after removal: {filtered_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e218dc00-5833-4083-9f19-a57810a3ef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (4055, 8)\n",
      "New shape after removal: (3740, 8)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"filtered_data_updated_4.csv\"  # Update with the correct path if needed\n",
    "filtered_data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove rows where 'solute_name' contains 'DONOTUSE'\n",
    "filtered_data = filtered_data[~filtered_data['solvent_name'].str.contains('PEG400/water', na=False)]\n",
    "\n",
    "# Save the cleaned data back to a new CSV file\n",
    "filtered_data.to_csv(\"filtered_data_updated_final.csv\", index=False)\n",
    "\n",
    "# Print shape to confirm row removal\n",
    "print(f\"Original shape: {pd.read_csv(file_path).shape}\")\n",
    "print(f\"New shape after removal: {filtered_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28859dc1-ae0f-4cf5-afbe-ddf7737153ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1.016600e+04\n",
      "mean    -1.062389e-16\n",
      "std      1.000049e+00\n",
      "min     -6.299726e+00\n",
      "25%     -4.444320e-01\n",
      "50%      1.684111e-01\n",
      "75%      7.319421e-01\n",
      "max      1.718791e+00\n",
      "Name: logS_M_normalized, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"filtered_data2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply Z-score normalization (standardization)\n",
    "df['logS_M_normalized'] = scaler.fit_transform(df[['logS_M']])\n",
    "\n",
    "# Save the normalized dataset\n",
    "df.to_csv(\"zscore_normalized_data.csv\", index=False)\n",
    "\n",
    "# Check the result (optional)\n",
    "print(df['logS_M_normalized'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf75ff4a-b682-45df-b9af-e619c3b1c3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
